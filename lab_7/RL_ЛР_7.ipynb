{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmOHHlg6MzSP"
   },
   "source": [
    "# Лабораторная работа 7. Инструментарий Stable Baselines 3. Алгоритмы, функции обратного вызова, запись видео, подбор значений гиперпараметров\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5TqD_5jrD0I"
   },
   "source": [
    "Ну что, ребята, вот и пришло наше время использовать \"фит + предикт\"!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC_EMNqlMaSN"
   },
   "source": [
    "В завершающей лабораторной работе мы познакомимся с классическим инструментом для решения задач обучения с подкреплением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u9piLlt1M8x"
   },
   "source": [
    "Мы решим 3 задачи в 3 окружениях. Эти окружения могут быть посложнее тех, которые мы использовали ранее. Первое решим с помощью DQN. Второе с помощью PPO - усовершенствованной версии алгоритма REINFORCE. И еще одно помощью DDPG, который рассматривался в предыдущей ЛР."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIOpKjMm4nh3"
   },
   "source": [
    "Крутость библиотеки Stable Baselines не только в эффективных реализациях различных продвинутых методов обучения с подкреплением, но и в том, что она предлагает дополнительные возможности для упрощения процесса обучения. Мы с вами познакомимся с функциями обратного вызова и запишем видео с агентом в окружении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNJO6g7FH8C3"
   },
   "source": [
    "Также, в этой лабораторной работе именно вы определяете реальность и все реализуете самостоятельно. Связано это с тем, что во многих местах вы будете заниматься копипастом. Вы можете создавать свои ячейки и писать код так, как хотите. Наша цель понять, как библиотека Stable Baselines позволяет нам быстро и удобно получать готовые модели для решения различных задач обучения с подкреплением. Поехали!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bA50wyh72Xa_"
   },
   "source": [
    "## Установка библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SIZ1ojXM2frA",
    "outputId": "e6271af1-b86e-463b-a8ee-ab30d037621f"
   },
   "outputs": [],
   "source": [
    "# %pip install stable-baselines3[extra]\n",
    "# %pip install box2d box2d-kengz\n",
    "# %pip install box2d-py\n",
    "# %pip install gym pyvirtualdisplay\n",
    "# %pip install pygame\n",
    "# %pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M87dlo_q22un"
   },
   "source": [
    "В этой работе вы будете часто обращаться к документации: https://stable-baselines3.readthedocs.io/en/master/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkuh6v6Y3AJn"
   },
   "source": [
    "## DQN и запись видео в Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTtGZGCQ9V1p"
   },
   "source": [
    "Создайте любое окружение, которое может быть решено с помощью DQN (в помощь вам и информация из документации к Stable Baselines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_tPA5j89tbY"
   },
   "source": [
    "Выполните обучение модели в окружении. Установите желаемое количество шагов для обучения. Сохраните модель после обучения на ваш Google Drive или скачайте ее на компьютер с помощью Colab (подсказка: from google.colab import files и т.д.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaZdCmQv-tT5"
   },
   "source": [
    "С помощью метода для оценки стратегии, оцените качество модели. Выполнить это нужно с помощью Stable Baselines. Сыграйте пару сотен игр и получите среднее значение и среднеквадратичное отклонение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zj5_Ecf_GFm"
   },
   "source": [
    "Запишите видео с работой обученной модели. Сохраните его на Google Drive или загрузите на компьютер. При проверке ЛР видео просматриваются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "251526ab853a45669225d0ff7ebeff87",
      "9e76ca506181432cbc1f840e1b846e14"
     ]
    },
    "id": "7I_hO7tf2yxo",
    "outputId": "c4a609de-5810-4cbb-e2fd-562e48803ed3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sega/.local/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward : -127.44996056969248\n",
      "std_reward : 52.22871020521623\n"
     ]
    }
   ],
   "source": [
    "# ваш код\n",
    "# нет, не мой))) https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
    "import gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Create environment\n",
    "env = DummyVecEnv([lambda: gym.make(\"LunarLander-v2\")])\n",
    "\n",
    "# env = VecVideoRecorder(env, 'video',\n",
    "#                        record_video_trigger=lambda x: x == 0, video_length=100,\n",
    "#                        name_prefix=f\"firstModel\")\n",
    "env.reset()\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN(\"MlpPolicy\", env)\n",
    "# Train the agent and display a progress bar\n",
    "# model.learn(total_timesteps=int(1e5))\n",
    "model.learn(total_timesteps=int(1e5))\n",
    "# Save the agent\n",
    "model.save(\"model/dqn_lunar\")\n",
    "del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the trained agent\n",
    "# NOTE: if you have loading issue, you can pass `print_system_info=True`\n",
    "# to compare the system on which the model was trained vs the current one\n",
    "# model = DQN.load(\"dqn_lunar\", env=env, print_system_info=True)\n",
    "model = DQN.load(\"model/dqn_lunar\", env=env)\n",
    "\n",
    "# Evaluate the agent\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "# # Enjoy trained agent\n",
    "# vec_env = model.get_env()\n",
    "# obs = vec_env.reset()\n",
    "# for i in range(1000):\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, rewards, dones, info = vec_env.step(action)\n",
    "#     vec_env.render()\n",
    "\n",
    "print('mean_reward', mean_reward, sep=' : ')\n",
    "print('std_reward', std_reward, sep=' : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PendulumEnv' object has no attribute 'num_envs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13131/1557243713.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m env = VecVideoRecorder(env, 'video',\n\u001b[1;32m      6\u001b[0m                        \u001b[0mrecord_video_trigger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                        name_prefix=\"firstModel\")\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, venv, video_folder, record_video_trigger, video_length, name_prefix)\u001b[0m\n\u001b[1;32m     32\u001b[0m     ):\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mVecEnvWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvenv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, venv, observation_space, action_space)\u001b[0m\n\u001b[1;32m    255\u001b[0m         VecEnv.__init__(\n\u001b[1;32m    256\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mnum_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mobservation_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0maction_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_space\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             )\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PendulumEnv' object has no attribute 'num_envs'"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "\n",
    "video_length=1000\n",
    "\n",
    "env = VecVideoRecorder(env, 'video',\n",
    "                       record_video_trigger=lambda x: x == 0, video_length=video_length,\n",
    "                       name_prefix=\"firstModel\")\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(video_length + 1):\n",
    "  action = model.predict(state)[0]\n",
    "  state, _, _, _ = env.step(action)\n",
    "# Save the video\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8n-FRch_iSG"
   },
   "source": [
    "## PPO и функции обратного вызова в Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtQzypAU_pGe"
   },
   "source": [
    "По аналогии с DQN - самостоятельно выберите любое окружение и обучите агента с помощью алгоритма PPO. В этот раз надо будет выполнить обучение 3 раза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNfUU7xr_2PV"
   },
   "source": [
    "В прошлый раз мы сохраняли модель после завершения обучения. В этот раз не делайте так!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFSR8gjYAg6d"
   },
   "source": [
    "В библиотеке Stable Baselines есть такая вещь как функции обратного вызова. Вы можете реализовывать свои, но уже есть несколько предопределенных. Найдите калбек, который будет сохранять вашу модель через каждые n шагов. Обучите модель с использованием такого калбека."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZNxRG9X_n1m",
    "outputId": "e17e4268-34f8-4fda-e1a7-338dcd35d0e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 803  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008913951 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.00773     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.36        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 53.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 609         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009452327 |\n",
      "|    clip_fraction        | 0.0712      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0675      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 32.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009775508 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.1        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 50.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 611          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075207474 |\n",
      "|    clip_fraction        | 0.0617       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.61        |\n",
      "|    explained_variance   | 0.288        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.2         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0168      |\n",
      "|    value_loss           | 57.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 620         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009970229 |\n",
      "|    clip_fraction        | 0.0859      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 0.561       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.7        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 57          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 632         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008286872 |\n",
      "|    clip_fraction        | 0.0534      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00934    |\n",
      "|    value_loss           | 60.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 638          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045506125 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | 0.583        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.8         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0107      |\n",
      "|    value_loss           | 51.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 643         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007783833 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.17        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 19.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 648          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042363694 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.549       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.24         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.005       |\n",
      "|    value_loss           | 25.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006376724 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.08        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00346    |\n",
      "|    value_loss           | 12.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005914026 |\n",
      "|    clip_fraction        | 0.0525      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.88        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00853    |\n",
      "|    value_loss           | 15.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 663         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007372586 |\n",
      "|    clip_fraction        | 0.0367      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.52       |\n",
      "|    explained_variance   | 0.00966     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.8        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00332    |\n",
      "|    value_loss           | 44.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f7949276790>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ваш код с обучением, последовательным сохранением промежуточных моделей и тестированием конечной модели\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=1000,\n",
    "  save_path=\"./logs/\",\n",
    "  name_prefix=\"rl_model\",\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True, # нужно для тех env которые используют VecNormalize\n",
    ")\n",
    "\n",
    "# Parallel environments\n",
    "env = DummyVecEnv([lambda: gym.make(\"CartPole-v1\")])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=25000, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38KIe8isBeUF"
   },
   "source": [
    "Теперь найдите калбек, который позволяет оценивать модель каждые n шагов и в итоге сохранить лучшую модель. Не забудьте указать папки для сохранения лучшей модели и для сохранения результатов тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EDJqxhlaBuZE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sega/.local/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=8.60 +/- 0.80\n",
      "Episode length: 8.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=8.40 +/- 0.49\n",
      "Episode length: 8.40 +/- 0.49\n",
      "Eval num_timesteps=1500, episode_reward=8.40 +/- 0.49\n",
      "Episode length: 8.40 +/- 0.49\n",
      "Eval num_timesteps=2000, episode_reward=8.80 +/- 0.75\n",
      "Episode length: 8.80 +/- 0.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2500, episode_reward=153.00 +/- 27.16\n",
      "Episode length: 153.00 +/- 27.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=174.40 +/- 48.78\n",
      "Episode length: 174.40 +/- 48.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3500, episode_reward=144.20 +/- 12.73\n",
      "Episode length: 144.20 +/- 12.73\n",
      "Eval num_timesteps=4000, episode_reward=143.40 +/- 18.10\n",
      "Episode length: 143.40 +/- 18.10\n",
      "Eval num_timesteps=4500, episode_reward=305.60 +/- 143.13\n",
      "Episode length: 305.60 +/- 143.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=292.20 +/- 146.30\n",
      "Episode length: 292.20 +/- 146.30\n",
      "Eval num_timesteps=5500, episode_reward=211.60 +/- 114.63\n",
      "Episode length: 211.60 +/- 114.63\n",
      "Eval num_timesteps=6000, episode_reward=394.60 +/- 129.36\n",
      "Episode length: 394.60 +/- 129.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6500, episode_reward=237.40 +/- 118.52\n",
      "Episode length: 237.40 +/- 118.52\n",
      "Eval num_timesteps=7000, episode_reward=224.60 +/- 139.79\n",
      "Episode length: 224.60 +/- 139.79\n",
      "Eval num_timesteps=7500, episode_reward=289.20 +/- 165.72\n",
      "Episode length: 289.20 +/- 165.72\n",
      "Eval num_timesteps=8000, episode_reward=278.00 +/- 118.33\n",
      "Episode length: 278.00 +/- 118.33\n",
      "Eval num_timesteps=8500, episode_reward=251.20 +/- 129.33\n",
      "Episode length: 251.20 +/- 129.33\n",
      "Eval num_timesteps=9000, episode_reward=305.80 +/- 116.97\n",
      "Episode length: 305.80 +/- 116.97\n",
      "Eval num_timesteps=9500, episode_reward=215.40 +/- 91.90\n",
      "Episode length: 215.40 +/- 91.90\n",
      "Eval num_timesteps=10000, episode_reward=332.80 +/- 116.00\n",
      "Episode length: 332.80 +/- 116.00\n",
      "Eval num_timesteps=10500, episode_reward=407.40 +/- 114.08\n",
      "Episode length: 407.40 +/- 114.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=426.80 +/- 91.38\n",
      "Episode length: 426.80 +/- 91.38\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11500, episode_reward=429.40 +/- 86.51\n",
      "Episode length: 429.40 +/- 86.51\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=386.00 +/- 93.20\n",
      "Episode length: 386.00 +/- 93.20\n",
      "Eval num_timesteps=12500, episode_reward=447.80 +/- 63.97\n",
      "Episode length: 447.80 +/- 63.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=466.80 +/- 66.40\n",
      "Episode length: 466.80 +/- 66.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13500, episode_reward=461.80 +/- 46.91\n",
      "Episode length: 461.80 +/- 46.91\n",
      "Eval num_timesteps=14000, episode_reward=432.40 +/- 59.47\n",
      "Episode length: 432.40 +/- 59.47\n",
      "Eval num_timesteps=14500, episode_reward=326.40 +/- 37.37\n",
      "Episode length: 326.40 +/- 37.37\n",
      "Eval num_timesteps=15000, episode_reward=392.40 +/- 91.84\n",
      "Episode length: 392.40 +/- 91.84\n",
      "Eval num_timesteps=15500, episode_reward=385.80 +/- 88.93\n",
      "Episode length: 385.80 +/- 88.93\n",
      "Eval num_timesteps=16000, episode_reward=381.80 +/- 86.22\n",
      "Episode length: 381.80 +/- 86.22\n",
      "Eval num_timesteps=16500, episode_reward=443.60 +/- 69.15\n",
      "Episode length: 443.60 +/- 69.15\n",
      "Eval num_timesteps=17000, episode_reward=462.80 +/- 49.60\n",
      "Episode length: 462.80 +/- 49.60\n",
      "Eval num_timesteps=17500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=489.60 +/- 20.80\n",
      "Episode length: 489.60 +/- 20.80\n",
      "Eval num_timesteps=18500, episode_reward=366.80 +/- 71.93\n",
      "Episode length: 366.80 +/- 71.93\n",
      "Eval num_timesteps=19000, episode_reward=460.60 +/- 49.97\n",
      "Episode length: 460.60 +/- 49.97\n",
      "Eval num_timesteps=19500, episode_reward=388.20 +/- 62.83\n",
      "Episode length: 388.20 +/- 62.83\n",
      "Eval num_timesteps=20000, episode_reward=472.00 +/- 27.63\n",
      "Episode length: 472.00 +/- 27.63\n",
      "Eval num_timesteps=20500, episode_reward=487.20 +/- 25.60\n",
      "Episode length: 487.20 +/- 25.60\n",
      "Eval num_timesteps=21000, episode_reward=489.60 +/- 16.67\n",
      "Episode length: 489.60 +/- 16.67\n",
      "Eval num_timesteps=21500, episode_reward=498.60 +/- 2.80\n",
      "Episode length: 498.60 +/- 2.80\n",
      "Eval num_timesteps=22000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=22500, episode_reward=498.80 +/- 2.40\n",
      "Episode length: 498.80 +/- 2.40\n",
      "Eval num_timesteps=23000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=23500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=24500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=25500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=26500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f78d06e7f50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ваш код с обучением, последовательным оцениванием промежуточных моделей и тестированием конечной модели\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "n = 500\n",
    "\n",
    "# чего он выводит: номер шага, среднюю награду, среднеквадратичное отклонение награды,\n",
    "# среднуюю длительность игры, среднеквадратичное отклонение длительности\n",
    "# ну понятно что награда будет совпадать с длительностью\n",
    "# потому что енв=cartpole и там за каждый шаг длительность +1 и награда +1\n",
    "# https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/common/callbacks.py\n",
    "eval_callback = EvalCallback(env, best_model_save_path=\"./logs/best-model\",\n",
    "                             log_path=\"./logs/\", eval_freq=n,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=25000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeDdX4QKBtu8"
   },
   "source": [
    "А теперь вы вправе использовать любые калбеки, которые хотите, но не менее двух. Stable Baselines позволяет обучать модель с несколькими калбеками. Изучите, как это происходит, и обучите еще одну модель с несколькими калбеками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEJn65LuCf49",
    "outputId": "fef8d5ac-eb07-443a-908e-237eaad394f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "step :>>  500 | reward :>> 28.0 (new max!)\n",
      "---\n",
      "step :>>  1000 | reward :>> 23.0 \n",
      "---\n",
      "step :>>  1500 | reward :>> 25.0 \n",
      "---\n",
      "step :>>  2000 | reward :>> 28.0 (new max!)\n",
      "---\n",
      "step :>>  2500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  3000 | reward :>> 363.0 \n",
      "---\n",
      "step :>>  3500 | reward :>> 311.0 \n",
      "---\n",
      "step :>>  4000 | reward :>> 414.0 \n",
      "---\n",
      "step :>>  4500 | reward :>> 125.0 \n",
      "---\n",
      "step :>>  5000 | reward :>> 75.0 \n",
      "---\n",
      "step :>>  5500 | reward :>> 114.0 \n",
      "---\n",
      "step :>>  6000 | reward :>> 75.0 \n",
      "---\n",
      "step :>>  6500 | reward :>> 111.0 \n",
      "---\n",
      "step :>>  7000 | reward :>> 139.0 \n",
      "---\n",
      "step :>>  7500 | reward :>> 386.0 \n",
      "---\n",
      "step :>>  8000 | reward :>> 388.0 \n",
      "---\n",
      "step :>>  8500 | reward :>> 218.0 \n",
      "---\n",
      "step :>>  9000 | reward :>> 264.0 \n",
      "---\n",
      "step :>>  9500 | reward :>> 243.0 \n",
      "---\n",
      "step :>>  10000 | reward :>> 483.0 \n",
      "---\n",
      "step :>>  10500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  11000 | reward :>> 152.0 \n",
      "---\n",
      "step :>>  11500 | reward :>> 269.0 \n",
      "---\n",
      "step :>>  12000 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  12500 | reward :>> 463.0 \n",
      "---\n",
      "step :>>  13000 | reward :>> 282.0 \n",
      "---\n",
      "step :>>  13500 | reward :>> 279.0 \n",
      "---\n",
      "step :>>  14000 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  14500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  15000 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  15500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  16000 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  16500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  17000 | reward :>> 281.0 \n",
      "---\n",
      "step :>>  17500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  18000 | reward :>> 381.0 \n",
      "---\n",
      "step :>>  18500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  19000 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  19500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  20000 | reward :>> 328.0 \n",
      "---\n",
      "step :>>  20500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  21000 | reward :>> 393.0 \n",
      "---\n",
      "step :>>  21500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  22000 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  22500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  23000 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  23500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  24000 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  24500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  25000 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  25500 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  26000 | reward :>> 500.0 (new max!)\n",
      "---\n",
      "step :>>  26500 | reward :>> 500.0 (new max!)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f7949caecd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ваш код с обучением, с несколькими калбеками на ваш выбор\n",
    "# КАЛбеки вызываются последовательно в том порядке, в котором были переданы\n",
    "# при прокидывании листа в параметр callback из него (листа в смысле) автоматически\n",
    "# формируется CallbackList\n",
    "# https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EveryNTimesteps\n",
    "\n",
    "class MyCoolCallback(BaseCallback):\n",
    "  def __init__(self, eval_env):\n",
    "    self.n_calls = 0\n",
    "    self.eval_env = eval_env\n",
    "    self.max_reward = -float('inf')\n",
    "  \n",
    "  def _on_step(self) -> bool:\n",
    "    \"\"\"\n",
    "    This method will be called by the model after each call to `env.step()`.\n",
    "    For child callback (of an `EventCallback`), this will be called\n",
    "    when the event is triggered.\n",
    "    :return: (bool) If the callback returns False, training is aborted early.\n",
    "    \"\"\"\n",
    "    reward, _ = evaluate_policy(self.model, self.eval_env,\n",
    "                                 n_eval_episodes=1,\n",
    "                                 render=False,\n",
    "                                 deterministic=True,\n",
    "                                 return_episode_rewards=True)\n",
    "    self.n_calls = self.n_calls + 1\n",
    "\n",
    "    print('---')\n",
    "    print('step :>> ', self.model.num_timesteps, '|', 'reward :>>', reward[0], end=' ')\n",
    "    if reward[0] >= self.max_reward:\n",
    "      self.max_reward = reward[0]\n",
    "      print('(new max!)')\n",
    "    else:\n",
    "      print()\n",
    "\n",
    "    return True\n",
    "\n",
    "every_n_timestep = EveryNTimesteps(500, MyCoolCallback(env))\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=25000, callback=[checkpoint_callback, every_n_timestep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kng2fg84Fdhb"
   },
   "source": [
    "## DDPG и подбор гиперпараметров в Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObUWUOmIFp-h"
   },
   "source": [
    "Выберите подходящее окружение. Обучите модель с использованием алгоритма DDPG. Оцените модель, сыграв пару сотен игр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U80fYFz2F0MB"
   },
   "source": [
    "Сохраните полученную модель на Google Drive или загрузите ее на свой компьютер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQSKhZxIF-Gw",
    "outputId": "4f11f046-16e1-4f28-a489-7c107b14cdca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.33e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 10        |\n",
      "|    fps             | 151       |\n",
      "|    time_elapsed    | 13        |\n",
      "|    total_timesteps | 2000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 53.4      |\n",
      "|    critic_loss     | 0.073     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1800      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.16e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 143       |\n",
      "|    time_elapsed    | 27        |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 86.5      |\n",
      "|    critic_loss     | 0.947     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3800      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -832     |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 77.1     |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -667     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 53.5     |\n",
      "|    critic_loss     | 2.71     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -558     |\n",
      "| time/              |          |\n",
      "|    episodes        | 50       |\n",
      "|    fps             | 138      |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35       |\n",
      "|    critic_loss     | 3.62     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9800     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ddpg.ddpg.DDPG at 0x7f78c8329490>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ваш код с обучением и тестированием модели, а также сохранением ее в Google Drive или на компьютере\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import DDPG\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwDcw7FLGpU3"
   },
   "source": [
    "Обратитесь к статье из документации, в которой описывается тюнинг модели: https://stable-baselines.readthedocs.io/en/master/guide/rl_zoo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4hIL9StG0Cv"
   },
   "source": [
    "Используйте эту информацию, чтобы обучить оптимальную DDPG-модель для вашего окружения. Используйте описанный в статье репозиторий. Поработайте с файлом train.py, чтобы подобрать оптимальные значения гиперпараметров. Используйте файл enjoy.py, чтобы протестировать вашу модель. Подсказка: обученные модели сохраняются в папку /logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PBzCQJ4HzXl",
    "outputId": "91218888-3c95-4e7d-a8d2-8a7d60eec09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stable-baselines in /home/sega/.local/lib/python3.7/site-packages (2.10.2)\n",
      "Requirement already satisfied: box2d in /home/sega/.local/lib/python3.7/site-packages (2.3.10)\n",
      "Collecting box2d-kengz\n",
      "  Using cached Box2D-kengz-2.3.3.tar.gz (425 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (5.4.1)\n",
      "Requirement already satisfied: pybullet in /home/sega/.local/lib/python3.7/site-packages (3.2.5)\n",
      "Collecting optuna\n",
      "  Using cached optuna-3.0.4-py3-none-any.whl (348 kB)\n",
      "Collecting pytablewriter\n",
      "  Using cached pytablewriter-0.64.2-py3-none-any.whl (106 kB)\n",
      "Requirement already satisfied: joblib in /home/sega/.local/lib/python3.7/site-packages (from stable-baselines) (1.1.0)\n",
      "Requirement already satisfied: cloudpickle>=0.5.5 in /home/sega/.local/lib/python3.7/site-packages (from stable-baselines) (2.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/sega/.local/lib/python3.7/site-packages (from stable-baselines) (3.5.1)\n",
      "Requirement already satisfied: scipy in /home/sega/.local/lib/python3.7/site-packages (from stable-baselines) (1.7.3)\n",
      "Requirement already satisfied: opencv-python in /home/sega/.local/lib/python3.7/site-packages (from stable-baselines) (4.5.5.64)\n",
      "Requirement already satisfied: numpy in /home/sega/.local/lib/python3.7/site-packages (from stable-baselines) (1.21.6)\n",
      "Requirement already satisfied: pandas in /home/sega/.local/lib/python3.7/site-packages (from stable-baselines) (1.3.5)\n",
      "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /home/sega/.local/lib/python3.7/site-packages (from stable-baselines) (0.21.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sega/.local/lib/python3.7/site-packages (from optuna) (21.3)\n",
      "Collecting colorlog\n",
      "  Using cached colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tqdm in /home/sega/.local/lib/python3.7/site-packages (from optuna) (4.64.0)\n",
      "Collecting cliff\n",
      "  Using cached cliff-3.10.1-py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: importlib-metadata<5.0.0 in /home/sega/.local/lib/python3.7/site-packages (from optuna) (4.13.0)\n",
      "Collecting sqlalchemy>=1.3.0\n",
      "  Using cached SQLAlchemy-1.4.44-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Using cached alembic-1.8.1-py3-none-any.whl (209 kB)\n",
      "Collecting cmaes>=0.8.2\n",
      "  Using cached cmaes-0.9.0-py3-none-any.whl (23 kB)\n",
      "Collecting tabledata<2,>=1.3.0\n",
      "  Using cached tabledata-1.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0\n",
      "  Using cached mbstrdecoder-1.1.1-py3-none-any.whl (7.7 kB)\n",
      "Collecting pathvalidate<3,>=2.3.0\n",
      "  Using cached pathvalidate-2.5.2-py3-none-any.whl (20 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5\n",
      "  Using cached tcolorpy-0.1.2-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /usr/lib/python3/dist-packages (from pytablewriter) (59.6.0)\n",
      "Collecting DataProperty<2,>=0.55.0\n",
      "  Using cached DataProperty-0.55.0-py3-none-any.whl (26 kB)\n",
      "Collecting typepy[datetime]<2,>=1.2.0\n",
      "  Using cached typepy-1.3.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: importlib-resources in /home/sega/.local/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (5.7.1)\n",
      "Requirement already satisfied: ale-py~=0.7.1 in /home/sega/.local/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines) (0.7.5)\n",
      "Requirement already satisfied: pyglet>=1.4.0 in /home/sega/.local/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines) (1.5.27)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sega/.local/lib/python3.7/site-packages (from importlib-metadata<5.0.0->optuna) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/sega/.local/lib/python3.7/site-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/lib/python3/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter) (4.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-2.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (514 kB)\n",
      "Requirement already satisfied: pytz>=2018.9 in /usr/lib/python3/dist-packages (from typepy[datetime]<2,>=1.2.0->pytablewriter) (2022.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /home/sega/.local/lib/python3.7/site-packages (from typepy[datetime]<2,>=1.2.0->pytablewriter) (2.8.2)\n",
      "Collecting PrettyTable>=0.7.2\n",
      "  Using cached prettytable-3.5.0-py3-none-any.whl (26 kB)\n",
      "Collecting cmd2>=1.0.0\n",
      "  Using cached cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
      "Collecting autopage>=0.4.0\n",
      "  Using cached autopage-0.5.1-py3-none-any.whl (29 kB)\n",
      "Collecting stevedore>=2.0.1\n",
      "  Using cached stevedore-3.5.2-py3-none-any.whl (50 kB)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Using cached pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/sega/.local/lib/python3.7/site-packages (from matplotlib->stable-baselines) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sega/.local/lib/python3.7/site-packages (from matplotlib->stable-baselines) (4.33.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/sega/.local/lib/python3.7/site-packages (from matplotlib->stable-baselines) (9.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sega/.local/lib/python3.7/site-packages (from matplotlib->stable-baselines) (0.11.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /home/sega/.local/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /home/sega/.local/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /home/sega/.local/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.8.0->typepy[datetime]<2,>=1.2.0->pytablewriter) (1.16.0)\n",
      "Building wheels for collected packages: box2d-kengz\n",
      "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[41 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Using setuptools (version 59.6.0).\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.7\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.7/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/__init__.py -> build/lib.linux-x86_64-3.7/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/Box2D.py -> build/lib.linux-x86_64-3.7/Box2D\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.7/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/b2/__init__.py -> build/lib.linux-x86_64-3.7/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'Box2D._Box2D' extension\n",
      "  \u001b[31m   \u001b[0m swigging Box2D/Box2D.i to Box2D/Box2D_wrap.cpp\n",
      "  \u001b[31m   \u001b[0m swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D/Box2D_wrap.cpp Box2D/Box2D.i\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:67: Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "  \u001b[31m   \u001b[0m Box2D/Box2D_math.i:47: Warning 302: %extend definition of 'b2Vec2'.\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:158: Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "  \u001b[31m   \u001b[0m Box2D/Box2D_math.i:168: Warning 302: %extend definition of 'b2Vec3'.\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:197: Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "  \u001b[31m   \u001b[0m Box2D/Box2D_math.i:301: Warning 302: %extend definition of 'b2Mat22'.\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:271: Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "  \u001b[31m   \u001b[0m Box2D/Box2D_math.i:372: Warning 302: %extend definition of 'b2Mat33'.\n",
      "  \u001b[31m   \u001b[0m Box2D/Collision/b2DynamicTree.h:44: Warning 312: Nested union not currently supported (ignored).\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Settings.h:144: Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:91: Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:85: Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Dynamics\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Dynamics/Contacts\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Dynamics/Joints\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Common\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Collision\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Collision/Shapes\n",
      "  \u001b[31m   \u001b[0m x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c Box2D/Box2D_wrap.cpp -o build/temp.linux-x86_64-3.7/Box2D/Box2D_wrap.o -I. -Wno-unused\n",
      "  \u001b[31m   \u001b[0m Box2D/Box2D_wrap.cpp:179:11: fatal error: Python.h: No such file or directory\n",
      "  \u001b[31m   \u001b[0m   179 | # include <Python.h>\n",
      "  \u001b[31m   \u001b[0m       |           ^~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m compilation terminated.\n",
      "  \u001b[31m   \u001b[0m error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for box2d-kengz\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-kengz\n",
      "Failed to build box2d-kengz\n",
      "Installing collected packages: box2d-kengz, tcolorpy, pbr, pathvalidate, mbstrdecoder, greenlet, colorlog, cmaes, autopage, typepy, stevedore, sqlalchemy, PrettyTable, cmd2, cliff, alembic, optuna, DataProperty, tabledata, pytablewriter\n",
      "  Running setup.py install for box2d-kengz ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for box2d-kengz\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[43 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Using setuptools (version 59.6.0).\n",
      "  \u001b[31m   \u001b[0m running install\n",
      "  \u001b[31m   \u001b[0m /usr/lib/python3/dist-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  \u001b[31m   \u001b[0m   setuptools.SetuptoolsDeprecationWarning,\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.7\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.7/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/__init__.py -> build/lib.linux-x86_64-3.7/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/Box2D.py -> build/lib.linux-x86_64-3.7/Box2D\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.7/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/b2/__init__.py -> build/lib.linux-x86_64-3.7/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'Box2D._Box2D' extension\n",
      "  \u001b[31m   \u001b[0m swigging Box2D/Box2D.i to Box2D/Box2D_wrap.cpp\n",
      "  \u001b[31m   \u001b[0m swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D/Box2D_wrap.cpp Box2D/Box2D.i\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:67: Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "  \u001b[31m   \u001b[0m Box2D/Box2D_math.i:47: Warning 302: %extend definition of 'b2Vec2'.\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:158: Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "  \u001b[31m   \u001b[0m Box2D/Box2D_math.i:168: Warning 302: %extend definition of 'b2Vec3'.\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:197: Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "  \u001b[31m   \u001b[0m Box2D/Box2D_math.i:301: Warning 302: %extend definition of 'b2Mat22'.\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:271: Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "  \u001b[31m   \u001b[0m Box2D/Box2D_math.i:372: Warning 302: %extend definition of 'b2Mat33'.\n",
      "  \u001b[31m   \u001b[0m Box2D/Collision/b2DynamicTree.h:44: Warning 312: Nested union not currently supported (ignored).\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Settings.h:144: Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:91: Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "  \u001b[31m   \u001b[0m Box2D/Common/b2Math.h:85: Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Dynamics\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Dynamics/Contacts\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Dynamics/Joints\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Common\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Collision\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.7/Box2D/Collision/Shapes\n",
      "  \u001b[31m   \u001b[0m x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c Box2D/Box2D_wrap.cpp -o build/temp.linux-x86_64-3.7/Box2D/Box2D_wrap.o -I. -Wno-unused\n",
      "  \u001b[31m   \u001b[0m Box2D/Box2D_wrap.cpp:179:11: fatal error: Python.h: No such file or directory\n",
      "  \u001b[31m   \u001b[0m   179 | # include <Python.h>\n",
      "  \u001b[31m   \u001b[0m       |           ^~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m compilation terminated.\n",
      "  \u001b[31m   \u001b[0m error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m box2d-kengz\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Cloning into 'rl-baselines3-zoo'...\n",
      "remote: Enumerating objects: 4563, done.\u001b[K\n",
      "remote: Total 4563 (delta 0), reused 0 (delta 0), pack-reused 4563\u001b[K\n",
      "Receiving objects: 100% (4563/4563), 2.91 MiB | 752.00 KiB/s, done.\n",
      "Resolving deltas: 100% (3017/3017), done.\n",
      "Submodule 'rl-trained-agents' (https://github.com/DLR-RM/rl-trained-agents) registered for path 'rl-trained-agents'\n",
      "Cloning into '/home/sega/study/reinforcement-learning/lab_7/rl-baselines3-zoo/rl-trained-agents'...\n",
      "remote: Enumerating objects: 2231, done.        \n",
      "remote: Counting objects: 100% (74/74), done.        \n",
      "remote: Compressing objects: 100% (54/54), done.        \n",
      "remote: Total 2231 (delta 20), reused 68 (delta 19), pack-reused 2157        \n",
      "Receiving objects: 100% (2231/2231), 1.35 GiB | 2.54 MiB/s, done.\n",
      "Resolving deltas: 100% (489/489), done.\n",
      "Submodule path 'rl-trained-agents': checked out '1e2a45e5d06efd6cc15da6cf2d1939d72dcbdf87'\n"
     ]
    }
   ],
   "source": [
    "# ваш код\n",
    "!apt-get install swig cmake libopenmpi-dev zlib1g-dev ffmpeg\n",
    "%pip install stable-baselines box2d box2d-kengz pyyaml pybullet optuna pytablewriter\n",
    "!git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItLaB1aS78JG",
    "outputId": "5de276bd-f5d7-4210-89b0-013b1ecc2125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow==1.13.2 in /home/sega/.local/lib/python3.7/site-packages (1.13.2)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow==1.13.2) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow==1.13.2) (1.13.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==1.13.2) (0.37.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow==1.13.2) (1.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow==1.13.2) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow==1.13.2) (1.44.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow==1.13.2) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow==1.13.2) (1.0.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow==1.13.2) (1.21.6)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow==1.13.2) (3.19.6)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow==1.13.2) (1.13.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow==1.13.2) (1.1.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow==1.13.2) (1.16.0)\n",
      "Requirement already satisfied: h5py in /home/sega/.local/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (3.6.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/sega/.local/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (2.1.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/sega/.local/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.6)\n",
      "Requirement already satisfied: mock>=2.0.0 in /home/sega/.local/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.2) (4.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/sega/.local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.13.0)\n",
      "Requirement already satisfied: cached-property in /home/sega/.local/lib/python3.7/site-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.2) (1.5.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/sega/.local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sega/.local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow==1.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAhubrDb66ER",
    "outputId": "fa71ec4a-8c64-498e-d18e-d5ebdeee27bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Pendulum-v1 ==========\n",
      "Seed: 1206920771\n",
      "Loading hyperparameters from: /home/sega/study/reinforcement-learning/lab_7/rl-baselines3-zoo/hyperparams/ddpg.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('buffer_size', 200000),\n",
      "             ('gamma', 0.98),\n",
      "             ('gradient_steps', -1),\n",
      "             ('learning_rate', 0.001),\n",
      "             ('learning_starts', 10000),\n",
      "             ('n_timesteps', 20000),\n",
      "             ('noise_std', 0.1),\n",
      "             ('noise_type', 'normal'),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[400, 300])'),\n",
      "             ('train_freq', [1, 'episode'])])\n",
      "Using 1 environments\n",
      "Creating test environment\n",
      "Applying normal noise with std 0.1\n",
      "Using cpu device\n",
      "Log path: logs/ddpg/Pendulum-v1_2\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.25e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 3950      |\n",
      "|    time_elapsed    | 0         |\n",
      "|    total_timesteps | 800       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.29e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 3973      |\n",
      "|    time_elapsed    | 0         |\n",
      "|    total_timesteps | 1600      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.28e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 4005      |\n",
      "|    time_elapsed    | 0         |\n",
      "|    total_timesteps | 2400      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.29e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 4027      |\n",
      "|    time_elapsed    | 0         |\n",
      "|    total_timesteps | 3200      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.29e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 4032      |\n",
      "|    time_elapsed    | 0         |\n",
      "|    total_timesteps | 4000      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.26e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 4045      |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 4800      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.25e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 4060      |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 5600      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.24e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 4051      |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 6400      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.23e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 4054      |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 7200      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.24e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 4059      |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 8000      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.21e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 4065      |\n",
      "|    time_elapsed    | 2         |\n",
      "|    total_timesteps | 8800      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -1.2e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 4068     |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 9600     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.23e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 52        |\n",
      "|    fps             | 2597      |\n",
      "|    time_elapsed    | 4         |\n",
      "|    total_timesteps | 10400     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 8.11      |\n",
      "|    critic_loss     | 2.62      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 200       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.23e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 56        |\n",
      "|    fps             | 1129      |\n",
      "|    time_elapsed    | 9         |\n",
      "|    total_timesteps | 11200     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 27.6      |\n",
      "|    critic_loss     | 0.0723    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1000      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.21e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 60        |\n",
      "|    fps             | 755       |\n",
      "|    time_elapsed    | 15        |\n",
      "|    total_timesteps | 12000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 44.5      |\n",
      "|    critic_loss     | 0.168     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1800      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.17e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 64        |\n",
      "|    fps             | 572       |\n",
      "|    time_elapsed    | 22        |\n",
      "|    total_timesteps | 12800     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 57.1      |\n",
      "|    critic_loss     | 0.308     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2600      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 68        |\n",
      "|    fps             | 479       |\n",
      "|    time_elapsed    | 28        |\n",
      "|    total_timesteps | 13600     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 63.9      |\n",
      "|    critic_loss     | 0.329     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3400      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.06e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 72        |\n",
      "|    fps             | 419       |\n",
      "|    time_elapsed    | 34        |\n",
      "|    total_timesteps | 14400     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 67.9      |\n",
      "|    critic_loss     | 0.616     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4200      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.01e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 76        |\n",
      "|    fps             | 371       |\n",
      "|    time_elapsed    | 40        |\n",
      "|    total_timesteps | 15200     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 69.9      |\n",
      "|    critic_loss     | 0.861     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 5000      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -963     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 343      |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 70.5     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -923     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 319      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 16800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 70.7     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6600     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -888     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 298      |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 17600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 68.2     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7400     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -858     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 282      |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 18400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 67.6     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8200     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -832     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 270      |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 19200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 65.8     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -805     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 259      |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 63.4     |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9800     |\n",
      "---------------------------------\n",
      "Saving to logs/ddpg/Pendulum-v1_2\n"
     ]
    }
   ],
   "source": [
    "!python3.7 rl-baselines3-zoo/train.py --algo ddpg --env Pendulum-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "import gym\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "model = DDPG.load(\"logs/ddpg/Pendulum-v1_2/Pendulum-v1.zip\", env=env)\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "  action = model.predict(state)[0]\n",
    "  state, _, done, _ = env.step(action)\n",
    "  if done: break;\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhdsxODoKT5M"
   },
   "source": [
    "Теперь вы знаете, как с помощью одной библиотеки и одного репозитория решать самые разные задачи. Напоминаю, что никто не запрещает вам создавать собственные окружения и решать задачи в них!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WC9-u_eOIrNV"
   },
   "source": [
    "## Последнее слово Гуненкова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40cbjKBGIe3N"
   },
   "source": [
    "<b>Дорогой студент, дорогой друг (подруга)!</b>\n",
    "<br/><br/>\n",
    "Хочу поздравить тебя с выполнением всех заданий этого экспериментального курса. Благодарю тебя за то, что ты не сдался (не сдалась) и дошел до конца. \n",
    "<br/><br/>\n",
    "Я надеюсь, что ты разобрался в основах обучения с подкреплением и теперь имеешь практические навыки в создании собственных крутых моделей. \n",
    "<br/><br/>\n",
    "Ты принадлежишь к числу тех, кто впервые прошел этот курс. Возможно, эти лабораторные больше никто не будет выполнять. А может быть, кафедра действительно будет их использовать для обучения на 4 курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crKudFtsJY9M"
   },
   "source": [
    "Эти лабораторные не сделаны по книгам или по урокам. Они сделаны человеком, который вместе с тобой погрузился в эту новую дисциплину.\n",
    "<br/><br/>\n",
    "<b>Если тебе есть что сказать именно по поводу этих заданий</b> (скорее всего тебя попросят пройти традиционные опросы по каждому преподавателю, но здесь это только между нами), прошу писать сюда: https://forms.gle/3nqvHbtAMo8qghmXA\n",
    "<br/><br/>\n",
    "Я проанализирую твое мнение и обязательно учту его в дальнейшей работе. Спасибо!\n",
    "<br/><br/>\n",
    "А теперь, нам пора прощаться.\n",
    "<br/><br/>\n",
    "<b>Желаю успешной подготовки и защиты ВКР и поступления в магистратуру</b> \n",
    "<br/><br/>\n",
    "До свидания! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6VWM8zJl_-y"
   },
   "source": [
    "## Задания лабораторной работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqAer6IQKMiz"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFUwXYjTnKbi"
   },
   "source": [
    "### Основные задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDtPQ8RfmFeq"
   },
   "source": [
    "1. Выполните все промежуточные задания. <b> 6 баллов </b>\n",
    "<br /><br />\n",
    "Итого за лабораторную работу: 6 баллов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdriwReLnvN5"
   },
   "source": [
    "### Замечания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jynEW3SUnxoZ"
   },
   "source": [
    "1. Выполнять задания можно в Google Colab или локально в среде Jupiter Notebook. При выполнении задания локально не требуется настройка и установка зависимостей для виртуального дисплея (и использование кастомной функции visualize). При вызове метода env.render создастся окно с визуализацией."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "251526ab853a45669225d0ff7ebeff87": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_9e76ca506181432cbc1f840e1b846e14",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">199,947/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:08:39</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">283 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m199,947/200,000 \u001b[0m [ \u001b[33m0:08:39\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m283 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "9e76ca506181432cbc1f840e1b846e14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
